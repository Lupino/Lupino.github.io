---
layout: post
title: "重拾的神经网络"
date: 2014-12-25 13:00
---

接触人工神经网络到现在也已经有四年了，从最基本的神经网络开始慢慢的到了现在的深度学习。
最近研究 [caffe](https://github.com/BVLC/caffe) 重拾记忆。

最简单的神经网络 -- 神经元
=====================

![神经元](/images/300px-SingleNeuron.png)

这个“神经元”是一个以 \textstyle x_1, x_2, x_3 及截距 \textstyle +1 为输入值的运算单元，其输出为 \textstyle  h_{W,b}(x) = f(W^Tx) = f(\sum_{i=1}^3 W_{i}x_i +b) ，其中函数 \textstyle f : \Re \mapsto \Re 被称为“激活函数”。

通常会使用的激活函数：

sigmoid函数

f(z) = \frac{1}{1+\exp(-z)}.

双曲正切函数（tanh）：

f(z) = \tanh(z) = \frac{e^z - e^{-z}}{e^z + e^{-z}}

神经网络模型
==========

所谓神经网络就是将许多个单一“神经元”联结在一起，这样，一个“神经元”的输出就可以是另一个“神经元”的输入。

![](/images/400px-Network331.png)

我们使用圆圈来表示神经网络的输入，标上“\textstyle +1”的圆圈被称为偏置节点，也就是截距项。神经网络最左边的一层叫做输入层，最右的一层叫做输出层（本例中，输出层只有一个节点）。中间所有节点组成的一层叫做隐藏层，因为我们不能在训练样本集中观测到它们的值。同时可以看到，以上神经网络的例子中有3个输入单元（偏置单元不计在内），3个隐藏单元及一个输出单元。

神经网络也可以有多个输出单元。

![](/images/500px-Network3322.png)

前向传播与方向传导
==============

梯度检验与高级优化
==============

自编码算法与稀疏性
==============

自编码神经网络是一种无监督学习算法，它使用了反向传播算法，并让目标值等于输入值，比如 \textstyle y^{(i)} = x^{(i)} 。

![](/images/400px-Autoencoder636.png)

参考 [UFLDL教程](http://deeplearning.stanford.edu/wiki/index.php/UFLDL教程)
